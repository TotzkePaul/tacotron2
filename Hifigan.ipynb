{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tacotron 2 inference code \n",
    "Edit the variables **checkpoint_path** and **text** to match yours and run the entire code to generate plots of mel outputs, alignments and audio synthesis from the generated mel-spectrogram using Griffin-Lim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and setup matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\deepfakes\\tacotron2-Offerman\\plotting_utils.py:2: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 362, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 265, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 302, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 539, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2714, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2818, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2878, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-892dcaffd16b>\", line 13, in <module>\n",
      "    import matplotlib.pylab as plt\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\matplotlib\\pylab.py\", line 252, in <module>\n",
      "    from matplotlib import cbook, mlab, pyplot as plt\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\matplotlib\\pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"C:\\Users\\DeepThought\\.conda\\envs\\inf\\lib\\site-packages\\matplotlib\\backends\\__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(\"Agg\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'init_weights' from 'utils' (E:\\deepfakes\\tacotron2-Offerman\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-892dcaffd16b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhifigan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttrDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhifigan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeldataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmel_spectrogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_WAV_VALUE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_wav\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhifigan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#from .denoiser import Denoiser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\deepfakes\\tacotron2-Offerman\\hifigan\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConv1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConvTranspose1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAvgPool1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mweight_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_weight_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspectral_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minit_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mLRELU_SLOPE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'init_weights' from 'utils' (E:\\deepfakes\\tacotron2-Offerman\\utils.py)"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "#%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hparams import create_hparams\n",
    "from model import Tacotron2\n",
    "from layers import TacotronSTFT, STFT\n",
    "from audio_processing import griffin_lim\n",
    "from train import load_model\n",
    "from text import text_to_sequence\n",
    "\n",
    "from hifigan.env import AttrDict\n",
    "from hifigan.meldataset import mel_spectrogram, MAX_WAV_VALUE, load_wav\n",
    "from hifigan.models import Generator\n",
    "\n",
    "#from .denoiser import Denoiser\n",
    "\n",
    "hparams = create_hparams()\n",
    "hparams.sampling_rate = 22050\n",
    "hparams.max_decoder_steps = 10000\n",
    "\n",
    "\n",
    "h = None\n",
    "device = None\n",
    "\n",
    "def get_taco_mel(text):\n",
    "    speaker = ('jej_checkpoint_904500_done', 'jej_waveglow890k_done')\n",
    "    checkpoint_path = \"../Models/\"+ speaker[0]\n",
    "    model = load_model(hparams)\n",
    "    model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
    "    _ = model.cuda().eval().half()\n",
    "    sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "    sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()\n",
    "    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
    "    return mel_outputs_postnet\n",
    "    \n",
    "\n",
    "\n",
    "def inference(checkpoint_file, output_dir, input_dir):\n",
    "    generator = Generator(h).to(device)\n",
    "        \n",
    "    state_dict_g = torch.load(checkpoint_file, map_location=device)\n",
    "    generator.load_state_dict(state_dict_g['generator'])\n",
    "\n",
    "    filelist = os.listdir(a.input_wavs_dir)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    generator.eval()\n",
    "    generator.remove_weight_norm()\n",
    "    with torch.no_grad():\n",
    "        for i, filname in enumerate(filelist):\n",
    "            print(\"loop\", i, filname)\n",
    "            x = get_taco_mel(\"Tell me the meaning\")\n",
    "            x = torch.FloatTensor(x).to(device)\n",
    "            y_g_hat = generator(x)\n",
    "            audio = y_g_hat.squeeze()\n",
    "            audio = audio * MAX_WAV_VALUE\n",
    "            audio = audio.cpu().numpy().astype('int16')\n",
    "\n",
    "            output_file = os.path.join(a.output_dir, os.path.splitext(filname)[0] + '_generated_e2e.wav')\n",
    "            write(output_file, h.sampling_rate, audio)\n",
    "            print(output_file)\n",
    "\n",
    "\n",
    "def start_inf():\n",
    "    print('Initializing Inference Process..')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_wavs_dir', default='test_files')\n",
    "    parser.add_argument('--output_dir', default='generated_files')\n",
    "    parser.add_argument('--checkpoint_file', default='./pretrained/LJ_FT_T2_V1/generator_v1')\n",
    "    a = parser.parse_args(\"\")\n",
    "\n",
    "    config_file = os.path.join(os.path.split(a.checkpoint_file)[0], 'config.json')\n",
    "    with open(config_file) as f:\n",
    "        data = f.read()\n",
    "\n",
    "    global h\n",
    "    json_config = json.loads(data)\n",
    "    h = AttrDict(json_config)\n",
    "\n",
    "    torch.manual_seed(h.seed)\n",
    "    global device\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(h.seed)\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        print(\"cpu inference\")\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    inference(a.checkpoint_file, a.output_dir, a.input_wavs_dir)\n",
    "    \n",
    "start_inf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import re\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "checkpoints = ['56500','60000', '60500', '63000']\n",
    "checkpoints = [ '84000']\n",
    "waveglows = ['400000']\n",
    "\n",
    "speakers = [('da_checkpoint_824800_done', 'da_waveglow_1516200'), ('jej_checkpoint_904500_done', 'jej_waveglow890k_done')]\n",
    "speakers = [('jej_checkpoint_904500_done', 'jej_waveglow890k_done')]\n",
    "settings_groups = [('a', 0.02, 0.666)]\n",
    "texts = []\n",
    "\n",
    "text_file = open(\"../Samples/Work.txt\", \"r\", encoding=\"utf8\")\n",
    "texts = text_file.readlines()\n",
    "for speaker in speakers:\n",
    "    checkpoint = speaker[0]\n",
    "    wg = speaker[1]\n",
    "    checkpoint_path = \"../Models/\"+ speaker[0]\n",
    "    model = load_model(hparams)\n",
    "    model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
    "    _ = model.cuda().eval().half()\n",
    "    waveglow_path = '../Models/'+speaker[1]\n",
    "    waveglow = torch.load(waveglow_path)['model']\n",
    "    waveglow.cuda().eval().half()\n",
    "    for k in waveglow.convinv:\n",
    "        k.float()\n",
    "    denoiser = Denoiser(waveglow)\n",
    "    complete_audio = AudioSegment.silent()\n",
    "    for index, text in enumerate(texts):\n",
    "        for settings in settings_groups:\n",
    "            sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "            sequence = torch.autograd.Variable(\n",
    "                torch.from_numpy(sequence)).cuda().long()\n",
    "            mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
    "            with torch.no_grad():\n",
    "                audio = waveglow.infer(mel_outputs_postnet, sigma=settings[2])\n",
    "            audio_denoised = denoiser(audio, strength=settings[1])[:, 0]\n",
    "            audio = ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate)\n",
    "            audio = AudioSegment(audio.data, frame_rate=22050, sample_width=2, channels=1)\n",
    "            output_dir = '../Samples/Work2/'\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            filename = output_dir+'{0:03d}'.format(index)+'_'+checkpoint+\"_wg\"+wg+'_'+re.sub(r'\\W+', '', text)[:30]+'_'+settings[0]+'.wav'\n",
    "            audio.export(filename, format=\"wav\")\n",
    "            silence = AudioSegment.silent(duration=250)\n",
    "            complete_audio = complete_audio.append(audio)\n",
    "            complete_audio = complete_audio.append(silence)\n",
    "    complete_audio.export(output_dir +speaker[0]+\"Work.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import re\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "checkpoints = ['56500','60000', '60500', '63000']\n",
    "checkpoints = [ '84000']\n",
    "waveglows = ['400000']\n",
    "\n",
    "speakers = [('da_checkpoint_824800_done', 'da_waveglow_1516200'), ('jej_checkpoint_904500_done', 'jej_waveglow890k_done')]\n",
    "speakers = [('jej_checkpoint_904500_done', 'jej_waveglow890k_done')]\n",
    "settings_groups = [('a', 0.02, 0.666)]\n",
    "texts = []\n",
    "\n",
    "text_file = open(\"../Samples/Work.txt\", \"r\", encoding=\"utf8\")\n",
    "texts = text_file.readlines()\n",
    "for speaker in speakers:\n",
    "    checkpoint = speaker[0]\n",
    "    wg = speaker[1]\n",
    "    checkpoint_path = \"../Models/\"+ speaker[0]\n",
    "    model = load_model(hparams)\n",
    "    model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
    "    _ = model.cuda().eval().half()\n",
    "    waveglow_path = '../Models/'+speaker[1]\n",
    "    waveglow = torch.load(waveglow_path)['model']\n",
    "    waveglow.cuda().eval().half()\n",
    "    for k in waveglow.convinv:\n",
    "        k.float()\n",
    "    denoiser = Denoiser(waveglow)\n",
    "    complete_audio = AudioSegment.silent()\n",
    "    for index, text in enumerate(texts):\n",
    "        for settings in settings_groups:\n",
    "            sequence = np.array(text_to_sequence(text, ['english_cleaners']))[None, :]\n",
    "            sequence = torch.autograd.Variable(\n",
    "                torch.from_numpy(sequence)).cuda().long()\n",
    "            mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n",
    "            with torch.no_grad():\n",
    "                audio = waveglow.infer(mel_outputs_postnet, sigma=settings[2])\n",
    "            audio_denoised = denoiser(audio, strength=settings[1])[:, 0]\n",
    "            audio = ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate)\n",
    "            audio = AudioSegment(audio.data, frame_rate=22050, sample_width=2, channels=1)\n",
    "            output_dir = '../Samples/Work2/'\n",
    "\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            filename = output_dir+'{0:03d}'.format(index)+'_'+checkpoint+\"_wg\"+wg+'_'+re.sub(r'\\W+', '', text)[:30]+'_'+settings[0]+'.wav'\n",
    "            audio.export(filename, format=\"wav\")\n",
    "            silence = AudioSegment.silent(duration=250)\n",
    "            complete_audio = complete_audio.append(audio)\n",
    "            complete_audio = complete_audio.append(silence)\n",
    "    complete_audio.export(output_dir +speaker[0]+\"Work.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-inf]",
   "language": "python",
   "name": "conda-env-.conda-inf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
